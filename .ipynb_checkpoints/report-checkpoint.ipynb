{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "#### Double Deep Q Network With Prioritized Experience Replay\n",
    "Experience Replay可以達到打破數據關聯性來幫助神經網路學習，naive的Experience Replay是使用均勻採樣的方式，但各transition的重要程度其實不盡相同，因此此作業使用Prioritized Experience Replay來改變DQN的Replay Memory中不同transition被sample到的機率，提高較重要之transition(TD-error高的transition)被sample到的機率，使得學習能夠更有效率，並使用Sum Tree來實作機率採樣。\n",
    "\n",
    "\n",
    "論文:https://arxiv.org/pdf/1511.05952.pdf\n",
    "\n",
    "![title](img/algo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game: \n",
    "Mountain Car (OpenAI gym)\n",
    "- Observation<br>\n",
    "\n",
    "| Num | Observation | Min   | Max  |\n",
    "| --- | ---         | ---   | ---  |\n",
    "| 0   | position    | -1.2  | 0.6  |\n",
    "| 1   | velocity    | -0.07 | 0.07 |\n",
    "\n",
    "- Reword:<br>\n",
    "    pos = (new_state[0] + 1.2) / 0.9 - 1 <br>\n",
    "    vel = abs(new_state[1]) / 0.035 -1 <br>\n",
    "    ( Clipped Car Position and abs(Car Velocity) within [−1, 1] .)<br>\n",
    "    reward = pos + vel <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setting\n",
    "Episode : 100<br>\n",
    "Timestep per Epoch : 500 (limited timestep in case the car can't climb to the target.)<br>\n",
    "learning_rate : 0.005<br>\n",
    "Optimizer: Adam\n",
    "\n",
    "## Replay Memory (Sum Tree):<br>\n",
    "Memory Size : 4000<br>\n",
    "Batch_size : 32<br>\n",
    "epsilon : 1.0<br>\n",
    "(epsilon-greedy)<br>\n",
    "epsilon_min : 0.01<br>\n",
    "epsilon_decay : 0.995<br>\n",
    "\n",
    "gamma : 0.85<br>\n",
    "alpha : 0.6<br>\n",
    "beta : 0.4 <br>\n",
    "epsilon : 0.01<br>\n",
    "(TD-error + epsilon prevent error is zero.)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "使用Double Deep Q Network With Prioritized Experience Replay<br>\n",
    "和僅作Random step 依據Episode所獲得的reward來作比較<br>\n",
    "(Prioritized Experience Replay方法Memory size在到達4000個transition前仍在收集經驗)<br>\n",
    "淺色面積部份為10次取樣的標準差\n",
    "\n",
    "1.Total Reward\n",
    "![title](img/result.png) \n",
    "\n",
    "2.Reward per step\n",
    "![title](img/result2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What kind of RL algorithms did you use? value-based, policy-based, model-based? why? \n",
    "使用Double Deep Q Network With Prioritized Experience Replay，為value-based的方法。<br>\n",
    "Experience Replay可以達到打破數據關聯性來幫助神經網路學習，再加上使用Prioritized Experience Replay，能使學習更加有效率。\n",
    "\n",
    "## 2. This algorithms is off-policy or on-policy? why? \n",
    "\n",
    "此方法為off-policy，因為是使用Replay Memory中的經驗來去學習並更新policy，而不是使用當前的policy。\n",
    "\n",
    "\n",
    "## 3. How does your algorithm solve the correlation problem in the same MDP? \n",
    "\n",
    "Experience Replay使用Replay Memory中的transition作機率採樣，此舉可以達到打破數據關聯性來幫助神經網路學習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
